import torch
import torch.nn as nn
import torch.nn.functional as F

class SEBlock(nn.Module):
    def __init__(self, in_channels, reduction=4):
        super().__init__()
        reduced_channels = in_channels // reduction
        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # グローバル平均プーリング
        self.fc = nn.Sequential(
            nn.Linear(in_channels, reduced_channels),
            nn.SiLU(),  # Swish活性化
            nn.Linear(reduced_channels, in_channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)      # (B, C)
        y = self.fc(y).view(b, c, 1, 1)      # (B, C, 1, 1)
        return x * y                         # スケーリング

class MBConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride, expand_ratio, kernel_size, se_ratio=0.25):
        super().__init__()
        self.use_residual = (stride == 1 and in_channels == out_channels)
        mid_channels = in_channels * expand_ratio

        layers = []
        if expand_ratio != 1:
            # Pointwise Conv (Expand)
            layers.append(nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False))
            layers.append(nn.BatchNorm2d(mid_channels))
            layers.append(nn.SiLU())

        # Depthwise Conv
        layers.append(nn.Conv2d(mid_channels, mid_channels, kernel_size=kernel_size, stride=stride,
                                padding=kernel_size // 2, groups=mid_channels, bias=False))
        layers.append(nn.BatchNorm2d(mid_channels))
        layers.append(nn.SiLU())

        # SE Block
        if se_ratio:
            layers.append(SEBlock(mid_channels, reduction=int(1/se_ratio)))

        # Pointwise Conv (Project)
        layers.append(nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False))
        layers.append(nn.BatchNorm2d(out_channels))

        self.block = nn.Sequential(*layers)

    def forward(self, x):
        out = self.block(x)
        if self.use_residual:
            return x + out
        else:
            return out

# (expand_ratio, channels, repeats, stride, kernel_size)
efficientnet_b0_config = [
    # ↓ block構成: (expand_ratio, out_channels, num_repeat, stride, kernel_size)
    (1, 16, 1, 1, 3),
    (6, 24, 2, 2, 3),
    (6, 40, 2, 2, 5),
    (6, 80, 3, 2, 3),
    (6, 112, 3, 1, 5),
    (6, 192, 4, 2, 5),
    (6, 320, 1, 1, 3),
]

class EfficientNetB0(nn.Module):
    def __init__(self, num_classes=1000):
        super().__init__()
        # Stem
        self.stem = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.SiLU()
        )

        # Build blocks
        self.blocks = nn.Sequential()
        in_channels = 32
        block_id = 0

        for expand_ratio, out_channels, repeats, stride, kernel_size in efficientnet_b0_config:
            for i in range(repeats):
                block_stride = stride if i == 0 else 1  # 最初のブロックだけstride使用
                block = MBConvBlock(
                    in_channels=in_channels,
                    out_channels=out_channels,
                    stride=block_stride,
                    expand_ratio=expand_ratio,
                    kernel_size=kernel_size
                )
                self.blocks.add_module(f"mbconv_{block_id}", block)
                in_channels = out_channels
                block_id += 1

        # Head
        self.head = nn.Sequential(
            nn.Conv2d(in_channels, 1280, kernel_size=1, bias=False),
            nn.BatchNorm2d(1280),
            nn.SiLU(),
            nn.AdaptiveAvgPool2d(1),  # グローバル平均プーリング
            nn.Flatten(),
            nn.Dropout(0.2),
            nn.Linear(1280, num_classes)
        )

    def forward(self, x):
        x = self.stem(x)
        x = self.blocks(x)
        x = self.head(x)
        return x
